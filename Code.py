# -*- coding: utf-8 -*-
"""DMR_projectChurn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PWhDBK5oLg2Bf8Pv4n8Ex9ABSOIzQG3c

# 1. **READ LIBRARIES**
"""

#Read libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn. metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier

"""# 2. **READ FILE**"""

#Read file Data.xlsx
df1 = pd.read_excel('Data.xlsx', sheet_name='Charges')
df2 = pd.read_excel('Data.xlsx', sheet_name='Other data')
df3 = pd.read_excel('Data.xlsx', sheet_name='Churn')
print("df1.shape", df1.shape)
print("df2.shape", df2.shape)
print("df3.shape", df3.shape)

"""# 3. **CLEAN DATA**"""

#Drop duplicates of column customerID
df1.drop_duplicates(subset='customerID', inplace=True)
df2.drop_duplicates(subset='customerID', inplace=True)
df3.drop_duplicates(subset='customerID', inplace=True)
print("df1.shape", df1.shape)
print("df2.shape", df2.shape)
print("df3.shape", df3.shape)

#Type of df1, df2 and df3
print("Charges:\n", df1.dtypes)
print("\nOther data:\n", df2.dtypes)
print("\nChurn: ", df3.dtypes)

#Change SeniorCitizen column to object type
df2['SeniorCitizen'] = df2['SeniorCitizen'].astype('object')

#Find missing values
print("Missing values sheet1\n", df1.isnull().sum())
print("Missing values sheet2\n", df2.isnull().sum())
print("Missing values sheet3\n", df3.isnull().sum())

#Replace empty strings with NaN before converting to float64
df1['TotalCharges'] = pd.to_numeric(df1['TotalCharges'], errors='coerce')
#Change TotalCharges column to float64
df1['TotalCharges'] = df1['TotalCharges'].astype('float64')
print(df1.dtypes)

#Fill missing values of MonthlyCharges dividing TotalCharges by df2 column tenure
df1.MonthlyCharges=df1.MonthlyCharges.fillna(df1['TotalCharges'] / df2['tenure'])
#Fill missing values of TotalCharges with a multiplication of tenure by MonthlyCharges
df1.TotalCharges=df1.TotalCharges.fillna(df2['tenure'] * df1['MonthlyCharges'])

"""# 4. **VISUALIZE DATA**"""

#Join table df1, df2 and df3
df=df1.join(df2.set_index('customerID'), on='customerID').join(df3.set_index('customerID'), on='customerID')
#Create main varibale and drop customerID column
df=df.drop(['customerID'], axis=1)

#Visualize categorical data
fig, ax = plt.subplots(5, 4, figsize=(20, 10))
for i, col in enumerate(df.columns):
    if df[col].dtype == 'object':
        sns.countplot(x=col, data=df, ax=ax[i//4, i%4],hue='Churn')
        ax[i//4, i%4].set_title(f'Countplot of {col}')
plt.tight_layout()
plt.show()

#Visualize numerical data using boxplot
fig, ax = plt.subplots(1, 3, figsize=(20, 6))
df.boxplot(column=['MonthlyCharges', 'TotalCharges','tenure'], by='Churn', ax=ax)
plt.show()

#Visualize numerical data using histogram
fig, ax = plt.subplots(1, 3, figsize=(20, 3))
df.hist(column=['MonthlyCharges', 'TotalCharges','tenure'], ax=ax)
plt.show()

"""# 5. **DATA PREPROCESSING**"""

#Transform dummy variable
df=pd.get_dummies(df,drop_first=True)

# Separate features and labels
X = df.drop('Churn_Yes', axis=1)
Y = df['Churn_Yes']

# Split data 70%-30% into training set and test set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

"""# 6. **MODELING**"""

#Define models
models={
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier(),
    'Neural Network': MLPClassifier()
}

#Scale data with RobustScaler
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# 7. **RESULTS**"""

# Train and evaluate each model
best_auc = 0
for name, model in models.items():
    model.fit(X_train, Y_train)
    predictions = model.predict(X_test)
    prob=model.predict_proba(X_test)
    print(name)
    print('Accuracy: ', accuracy_score(Y_test, predictions))
    print("Overall Precision:", precision_score(Y_test, predictions))
    print("Overall Recall:", recall_score(Y_test, predictions))
    print(confusion_matrix(Y_test, predictions))
    print("AUC: ", roc_auc_score(Y_test, prob[:,1]))
    print("\n")
    #Find the model with the highest AUC
    if roc_auc_score(Y_test, prob[:,1]) > best_auc:
        best_auc = roc_auc_score(Y_test, prob[:,1])
        best_model= model
        best_model_name = name

"""# 8. **BOOST**"""

#Improve parameters for best model
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
grid_search = GridSearchCV(best_model, param_grid, cv=5)
grid_search.fit(X_train, Y_train)
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)

#Train best model with best parameter
best_model = best_model.set_params(C=grid_search.best_params_['C'])
model = best_model.fit(X_train, Y_train)
predictions = model.predict(X_test)
prob=model.predict_proba(X_test)
print('Accuracy: ', accuracy_score(Y_test, predictions))
print("Overall Precision:", precision_score(Y_test, predictions))
print("Overall Recall:", recall_score(Y_test, predictions))
print(confusion_matrix(Y_test, predictions))
print("AUC: ", roc_auc_score(Y_test, prob[:,1]))